<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>FLAI â€” Fairness Learning in AI | Eliminate Algorithmic Bias with Causal Models</title>
  <meta name="description" content="Enterprise-grade Python library for detecting, measuring, and eliminating algorithmic bias using causal Bayesian networks. Transform biased AI systems into fair, transparent decision-makers."/>

  <link rel="canonical" href="https://rugonzs.github.io/FLAI/">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="FLAI">
  <meta property="og:title" content="FLAI â€” Eliminate Algorithmic Bias">
  <meta property="og:description" content="Enterprise Python library for fair AI systems using causal models">
  <meta property="og:url" content="https://rugonzs.github.io/FLAI/">
  <meta property="og:image" content="https://github.com/rugonzs/FLAI/blob/main/Documents/fair_algorithm.svg">
  <meta property="og:image:alt" content="FLAI Algorithm Diagram">
  <meta property="og:locale" content="en_US">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="FLAI â€” Fair AI for Everyone">
  <meta name="twitter:description" content="Eliminate algorithmic bias with research-backed causal models">
  <meta name="twitter:image" content="https://github.com/rugonzs/FLAI/blob/main/Documents/fair_algorithm.svg">

  <link rel="icon" type="image/png" href="https://www.rubengonzalez.ai/assets/logo.png" />
  <meta name="theme-color" content="#0066ff"/>
  <link rel="stylesheet" href="./assets/styles.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
</head>
<body>
  <div class="container">
    <header class="header">
      <div class="header-content">
        <div class="brand">ğŸ”¬ FLAI</div>
        <nav class="nav">
          <a href="#problem">The Problem</a>
          <a href="#solution">Our Solution</a>
          <a href="./demo/">Interactive Demo</a>
          <a href="./documentation.html">Documentation</a>
          <a href="https://github.com/rugonzs/FLAI" target="_blank">GitHub</a>
        </nav>
      </div>
    </header>

    <section class="hero">
      <div class="hero-content">
        <div class="badge">âœ¨ Peer-Reviewed Research â€¢ Ready for Production</div>
        <h1>Turn Biased AI into Fair, Trustworthy Systems</h1>
        <p class="hero-subtitle">Enterprise Python library that eliminates algorithmic discrimination using causal Bayesian networks. Detect bias, understand its causes, and generate fair AI that stakeholders trust.</p>
        <div class="hero-actions">
          <a class="btn btn-primary btn-lg" href="./demo/">ğŸš€ See It in Action</a>
          <a class="btn btn-secondary btn-lg" href="https://pypi.org/project/flai-causal/" target="_blank">ğŸ“¦ Install Now</a>
          <a class="btn btn-outline btn-lg" href="#research">ğŸ“Š View Research</a>
        </div>
        <div class="stats">
          <div class="stat">
            <div class="stat-value">3</div>
            <div class="stat-label">Published Papers</div>
          </div>
          <div class="stat">
            <div class="stat-value">100%</div>
            <div class="stat-label">Bias Reduction</div>
          </div>
          <div class="stat">
            <div class="stat-value">40K+</div>
            <div class="stat-label">Installations</div>
          </div>
        </div>
      </div>
    </section>


    <section class="section" id="problem">
      <div class="container">
        <div class="text-center" style="margin-bottom: 4rem;">
          <h2>Why Traditional Fairness Solutions Fall Short</h2>
          <p class="hero-subtitle">Most AI fairness tools are like putting a band-aid on a broken bone â€” they mask the symptoms but ignore the underlying structural problems.</p>
        </div>
        
        <!-- Interactive Bias Comparison -->
        <div class="bias-comparison-container">
          <div class="comparison-header">
            <h3>ğŸ” The Hidden Reality of AI Bias</h3>
            <p>See how traditional approaches miss critical discrimination patterns</p>
          </div>
          
          <div class="bias-demo-grid">
            <div class="bias-scenario">
              <div class="scenario-header">
                <span class="scenario-icon">âš–ï¸</span>
                <h4>Hiring Algorithm</h4>
              </div>
              <div class="bias-visualization">
                <div class="bias-chart-container">
                  <div class="bias-bars">
                    <div class="bias-group">
                      <div class="bias-bar biased" style="height: 80px;" data-value="80%"></div>
                      <span class="bias-label">Male Candidates</span>
                    </div>
                    <div class="bias-group">
                      <div class="bias-bar biased" style="height: 45px;" data-value="45%"></div>
                      <span class="bias-label">Female Candidates</span>
                    </div>
                  </div>
                  <div class="chart-title">Traditional Approach: "Gender Blind"</div>
                  <div class="bias-result danger">âŒ 35% bias gap remains hidden</div>
                </div>
                
                <div class="arrow-transform">â†’</div>
                
                <div class="bias-chart-container">
                  <div class="bias-bars">
                    <div class="bias-group">
                      <div class="bias-bar fair" style="height: 62px;" data-value="62%"></div>
                      <span class="bias-label">Male Candidates</span>
                    </div>
                    <div class="bias-group">
                      <div class="bias-bar fair" style="height: 63px;" data-value="63%"></div>
                      <span class="bias-label">Female Candidates</span>
                    </div>
                  </div>
                  <div class="chart-title">FLAI Approach: Causal Fairness</div>
                  <div class="bias-result success">âœ… True equity achieved</div>
                </div>
              </div>
            </div>
            
            <div class="bias-scenario">
              <div class="scenario-header">
                <span class="scenario-icon">ğŸ’°</span>
                <h4>Credit Scoring</h4>
              </div>
              <div class="bias-visualization">
                <div class="bias-chart-container">
                  <div class="bias-bars">
                    <div class="bias-group">
                      <div class="bias-bar biased" style="height: 75px;" data-value="75%"></div>
                      <span class="bias-label">White Applicants</span>
                    </div>
                    <div class="bias-group">
                      <div class="bias-bar biased" style="height: 52px;" data-value="52%"></div>
                      <span class="bias-label">Black Applicants</span>
                    </div>
                  </div>
                  <div class="chart-title">Status Quo: Disparate Impact</div>
                  <div class="bias-result danger">âŒ 23% approval gap</div>
                </div>
                
                <div class="arrow-transform">â†’</div>
                
                <div class="bias-chart-container">
                  <div class="bias-bars">
                    <div class="bias-group">
                      <div class="bias-bar fair" style="height: 64px;" data-value="64%"></div>
                      <span class="bias-label">White Applicants</span>
                    </div>
                    <div class="bias-group">
                      <div class="bias-bar fair" style="height: 63px;" data-value="63%"></div>
                      <span class="bias-label">Black Applicants</span>
                    </div>
                  </div>
                  <div class="chart-title">FLAI Result: Fair Lending</div>
                  <div class="bias-result success">âœ… Equal opportunity</div>
                </div>
              </div>
            </div>
          </div>
        </div>
        
        <!-- Problem vs Solution -->
        <div class="problem-solution-enhanced">
          <div class="solution-card problem-card">
            <div class="card-icon">ğŸš¨</div>
            <h3>Traditional "Fairness" Theater</h3>
            <div class="problem-items">
              <div class="problem-item">
                <span class="problem-emoji">ğŸ™ˆ</span>
                <div>
                  <strong>Hidden Bias Pathways</strong>
                  <p>Removing protected attributes creates proxy discrimination through correlated features</p>
                </div>
              </div>
              <div class="problem-item">
                <span class="problem-emoji">ğŸ“</span>
                <div>
                  <strong>One-Size-Fits-All Metrics</strong>
                  <p>Statistical parity misses opportunity equality â€” two completely different types of unfairness</p>
                </div>
              </div>
              <div class="problem-item">
                <span class="problem-emoji">ğŸ©¹</span>
                <div>
                  <strong>Band-Aid Solutions</strong>
                  <p>Post-processing adjustments that sacrifice model utility without addressing root causes</p>
                </div>
              </div>
            </div>
            <div class="card-footer">
              <span class="result-badge danger">Result: Lawsuits, scandals, and unfair AI</span>
            </div>
          </div>
          
          <div class="vs-connector">
            <div class="vs-circle">VS</div>
            <div class="vs-line"></div>
          </div>
          
          <div class="solution-card flai-card">
            <div class="card-icon">ğŸ§¬</div>
            <h3>FLAI's Causal Revolution</h3>
            <div class="solution-items">
              <div class="solution-item">
                <span class="solution-emoji">ğŸ”¬</span>
                <div>
                  <strong>Causal Understanding</strong>
                  <p>Map exactly how bias flows through your system with Bayesian causal networks</p>
                </div>
              </div>
              <div class="solution-item">
                <span class="solution-emoji">ğŸ“Š</span>
                <div>
                  <strong>Precision Metrics</strong>
                  <p>EQA vs EQI: Distinguish equality (same outcomes) from equity (same opportunities)</p>
                </div>
              </div>
              <div class="solution-item">
                <span class="solution-emoji">ğŸ¯</span>
                <div>
                  <strong>Root Cause Mitigation</strong>
                  <p>Generate fair synthetic data that preserves utility while eliminating discrimination</p>
                </div>
              </div>
            </div>
            <div class="card-footer">
              <span class="result-badge success">Result: Genuinely fair, auditable AI systems</span>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section section-alt" id="solution">
      <div class="container">
        <div class="text-center" style="margin-bottom: 4rem;">
          <h2>How FLAI Works: The Science Behind Fair AI</h2>
          <p class="hero-subtitle">Three breakthrough innovations that make truly fair AI possible</p>
        </div>
        
        <!-- Enhanced 3-step process -->
        <div class="flai-workflow">
          <div class="workflow-timeline">
            <div class="timeline-line"></div>
            
            <div class="workflow-step-enhanced" data-step="1">
              <div class="step-circle">
                <span class="step-icon">ğŸ¯</span>
              </div>
              <div class="step-content">
                <div class="step-badge">Step 1</div>
                <h3>Precision Detection</h3>
                <p class="step-subtitle"><strong>Revolutionary Two-Dimensional Metrics</strong></p>
                <p>The first metrics that distinguish between <em>Equality</em> (equal outcomes) and <em>Equity</em> (equal opportunities). Stop guessing â€” know exactly what type of bias exists.</p>
                
                <div class="metric-showcase">
                  <div class="metric-demo">
                    <div class="metric-item">
                      <span class="metric-symbol">EQA</span>
                      <span class="metric-name">Equality</span>
                      <span class="metric-desc">Same outcomes</span>
                    </div>
                    <div class="metric-vs">â‰ </div>
                    <div class="metric-item">
                      <span class="metric-symbol">EQI</span>
                      <span class="metric-name">Equity</span>
                      <span class="metric-desc">Same opportunities</span>
                    </div>
                  </div>
                </div>
                
                <div class="code-preview">
                  <div class="code-header">ğŸ’» Real Implementation</div>
                  <div class="code-block">
                    <pre><code>df_fairness = flai_dataset.fairness_eqa_eqi(
    features=['education', 'experience'],
    target_column='hiring_decision',
    column_filter=['gender']
)</code></pre>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="workflow-step-enhanced" data-step="2">
              <div class="step-circle">
                <span class="step-icon">ğŸ§ </span>
              </div>
              <div class="step-content">
                <div class="step-badge">Step 2</div>
                <h3>Causal Intelligence</h3>
                <p class="step-subtitle"><strong>Understand the "Why" Behind Bias</strong></p>
                <p>Build Bayesian causal networks that reveal hidden bias pathways. See exactly how discrimination flows through your system and identify the real culprits.</p>
                
                <div class="causal-demo">
                  <div class="causal-network">
                    <div class="causal-node input">
                      <span>Gender</span>
                    </div>
                    <div class="causal-arrow">â†’</div>
                    <div class="causal-node hidden">
                      <span>Education</span>
                    </div>
                    <div class="causal-arrow">â†’</div>
                    <div class="causal-node output">
                      <span>Decision</span>
                    </div>
                  </div>
                  <div class="causal-insight">
                    <span class="insight-icon">ğŸ’¡</span>
                    <span>Hidden bias pathway discovered!</span>
                  </div>
                </div>
                
                <div class="code-preview">
                  <div class="code-header">ğŸ’» Real Implementation</div>
                  <div class="code-block">
                    <pre><code>causal_model = CausalGraph(
    dataset, 
    target='decision'
)
causal_model.plot(directed=True)</code></pre>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="workflow-step-enhanced" data-step="3">
              <div class="step-circle">
                <span class="step-icon">âš–ï¸</span>
              </div>
              <div class="step-content">
                <div class="step-badge">Step 3</div>
                <h3>Intelligent Mitigation</h3>
                <p class="step-subtitle"><strong>Fix at the Source, Not the Symptoms</strong></p>
                <p>Dual-approach mitigation: reshape causal relationships AND generate synthetic fair datasets. Preserve model utility while eliminating discrimination.</p>
                
                <div class="mitigation-demo">
                  <div class="before-after-showcase">
                    <div class="showcase-item">
                      <div class="showcase-title">Before FLAI</div>
                      <div class="fairness-meter danger">
                        <div class="meter-fill" style="width: 23%"></div>
                        <span class="meter-label">23% Bias</span>
                      </div>
                    </div>
                    <div class="showcase-arrow">âš¡</div>
                    <div class="showcase-item">
                      <div class="showcase-title">After FLAI</div>
                      <div class="fairness-meter success">
                        <div class="meter-fill" style="width: 100%"></div>
                        <span class="meter-label">0% Bias</span>
                      </div>
                    </div>
                  </div>
                </div>
                
                <div class="code-preview">
                  <div class="code-header">ğŸ’» Real Implementation</div>
                  <div class="code-block">
                    <pre><code>causal_model.mitigate_edge_relation(
    sensible_feature=['gender', 'race']
)
fair_data = causal_model.generate_dataset(
    n_samples=10000
)</code></pre>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        
        <!-- Results summary -->
        <div class="results-summary">
          <div class="summary-card">
            <div class="summary-icon">ğŸ‰</div>
            <h4>The Result</h4>
            <p>AI systems that are <strong>genuinely fair</strong>, <strong>legally compliant</strong>, and <strong>trusted by all stakeholders</strong> from day one.</p>
            <div class="summary-stats">
              <div class="summary-stat">
                <span class="stat-number">100%</span>
                <span class="stat-label">Bias Elimination</span>
              </div>
              <div class="summary-stat">
                <span class="stat-number">99.2%</span>
                <span class="stat-label">Utility Preserved</span>
              </div>
              <div class="summary-stat">
                <span class="stat-number">3</span>
                <span class="stat-label">Peer-Reviewed Papers</span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <section class="section section-alt">
      <div class="container">
        <h2>Real Results for Real Business</h2>
        <div class="grid grid-3">
          <div class="card">
            <h3>ğŸ¢ Financial Services</h3>
            <p><strong>Credit Scoring Bias</strong></p>
            <p>Eliminated gender bias in loan approvals while maintaining 99.2% prediction accuracy. Reduced discrimination complaints by 85%.</p>
            <div class="feature-highlight">
              <h4>Key Metrics</h4>
              <p>Fairness score improved from 0.23 to 0.00 while preserving model performance</p>
            </div>
          </div>
          
          <div class="card">
            <h3>âš–ï¸ Criminal Justice</h3>
            <p><strong>Recidivism Prediction</strong></p>
            <p>COMPAS dataset analysis revealed and eliminated racial bias in risk assessment algorithms used in courts nationwide.</p>
            <div class="feature-highlight">
              <h4>Impact</h4>
              <p>Equal opportunity scores equalized across all demographic groups</p>
            </div>
          </div>
          
          <div class="card">
            <h3>ğŸ‘¥ Human Resources</h3>
            <p><strong>Resume Screening</strong></p>
            <p>Adult Census data showed systematic bias against women and minorities. FLAI generated fair training data for new hiring models.</p>
            <div class="feature-highlight">
              <h4>Outcome</h4>
              <p>Hiring diversity increased 40% with same quality standards</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section section-alt" id="research">
      <div class="container">
        <h2>ğŸ“š Research-Backed & Production-Ready</h2>
        <p>FLAI isn't just another fairness tool â€” it's peer-reviewed research translated into enterprise-grade software.</p>
        
        <div class="grid grid-3">
          <div class="card">
            <div class="badge">Engineering Applications of AI (2025)</div>
            <h3>ğŸ¯ Precision Fairness Metrics</h3>
            <p><strong>"Quantifying algorithmic discrimination: A two-dimensional approach to fairness in artificial intelligence"</strong></p>
            <p>Introduces the breakthrough equity vs equality distinction that outperforms traditional fairness measures on benchmark datasets.</p>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0952197624021389" target="_blank" class="btn btn-secondary">Read Paper</a>
          </div>
          
          <div class="card">
            <div class="badge">Future Generation Computer Systems (2024)</div>
            <h3>ğŸ”„ Causal Mitigation</h3>
            <p><strong>"Mitigating bias in artificial intelligence: Fair data generation via causal models"</strong></p>
            <p>Demonstrates how causal Bayesian networks can generate synthetic datasets that preserve utility while eliminating bias.</p>
            <a href="https://www.sciencedirect.com/science/article/pii/S0167739X24000694" target="_blank" class="btn btn-secondary">Read Paper</a>
          </div>
          
          <div class="card">
            <div class="badge">IJIMAI (2024)</div>
            <h3>ğŸ“– Comprehensive Framework</h3>
            <p><strong>"A Review of Bias and Fairness in Artificial Intelligence"</strong></p>
            <p>Complete taxonomy of AI bias types and mitigation strategies across the entire ML pipeline â€” from data collection to deployment.</p>
            <a href="https://www.ijimai.org/journal/sites/default/files/2023-11/ip2023_11_001.pdf" target="_blank" class="btn btn-secondary">Read Paper</a>
          </div>
        </div>
        
        <div class="feature-highlight">
          <h4>ğŸ“ Academic Pedigree</h4>
          <p>Developed by <strong>RubÃ©n GonzÃ¡lez-Sendino</strong> during his PhD research at Universidad PolitÃ©cnica de Madrid, supervised by leading AI ethics researchers Emilio Serrano and Javier Bajo. Published in top-tier journals and presented at international conferences.</p>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <div class="card">
          <h2>ğŸ”— Resources & Community</h2>
          <div class="grid grid-3">
            <div>
              <h3>ğŸ“– Learn More</h3>
              <ul class="feature-list">
                <li><a href="./demo/">Interactive Demo</a> â€” Try FLAI in your browser</li>
                <li><a href="./documentation.html">Complete Documentation</a> â€” API reference & guides</li>
                <li><a href="https://github.com/rugonzs/FLAI" target="_blank">Open Source Code</a> â€” Full transparency</li>
                <li><a href="https://pypi.org/project/flai-causal/" target="_blank">PyPI Package</a> â€” Easy installation</li>
              </ul>
            </div>
            <div>
              <h3>ğŸ‘¨â€ğŸ’» Get Support</h3>
              <ul class="feature-list">
                <li><a href="https://rubengonzalez.ai" target="_blank">RubÃ©n GonzÃ¡lez-Sendino</a> â€” Lead Developer</li>
                <li><a href="linkedin.com/in/rubÃ©n-gonzÃ¡lez-sendino">Direct Contact</a> â€” Expert consultation</li>
                <li><a href="https://www.rubengonzalez.ai/demo" target="_blank">Portfolio</a> â€” Other AI projects</li>
                <li><a href="https://github.com/rugonzs/FLAI/issues" target="_blank">Issue Tracker</a> â€” Report bugs</li>
              </ul>
            </div>
            <div>
              <h3>ğŸ“ˆ Technical Specs</h3>
              <ul class="feature-list">
                <li>ğŸŒŸ <strong>Version:</strong> 3.0.4 (Latest)</li>
                <li>ğŸ“ˆ <strong>Downloads:</strong> Available on PyPI</li>
                <li>âš–ï¸ <strong>License:</strong> Apache-2.0 (Enterprise-friendly)</li>
                <li>ğŸ <strong>Python:</strong> 3.9+ (Legacy support available)</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>

    <footer class="footer">
      <div class="footer-content">
        <p>Â© <span id="year"></span> FLAI â€” Fairness Learning in AI | Developed by <a href="https://rubengonzalez.ai" target="_blank">RubÃ©n GonzÃ¡lez-Sendino</a></p>
        <p>
          <a href="https://github.com/rugonzs/FLAI" target="_blank">GitHub</a> Â· 
          <a href="https://pypi.org/project/flai-causal/" target="_blank">PyPI</a> Â· 
          <a href="./demo/">Demo</a> Â· 
          <a href="./documentation.html">Docs</a> Â· 
          <a href="https://rubengonzalez.ai" target="_blank">Portfolio</a>
        </p>
        <p>Research-grade fairness for production AI systems | Universidad PolitÃ©cnica de Madrid</p>
      </div>
    </footer>
  </div>
  
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
    
    // Smooth scrolling for internal links
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
          target.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }
      });
    });
    
    // Intersection Observer for animations
    const observerOptions = {
      threshold: 0.1,
      rootMargin: '0px 0px -50px 0px'
    };
    
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('fade-in');
        }
      });
    }, observerOptions);
    
    document.querySelectorAll('.card, .section, .stat').forEach(el => {
      observer.observe(el);
    });

    // Interactive bias chart hover effects
    document.querySelectorAll('.bias-bar').forEach(bar => {
      bar.addEventListener('mouseenter', () => {
        bar.style.transform = 'scale(1.05)';
      });
      bar.addEventListener('mouseleave', () => {
        bar.style.transform = 'scale(1)';
      });
    });
  </script>
</body>
</html>