<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>FLAI â€” Fairness Learning in AI | Eliminate Algorithmic Bias with Causal Models</title>
  <meta name="description" content="Enterprise-grade Python library for detecting, measuring, and eliminating algorithmic bias using causal Bayesian networks. Transform biased AI systems into fair, transparent decision-makers."/>

  <link rel="canonical" href="https://rugonzs.github.io/FLAI/">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="FLAI">
  <meta property="og:title" content="FLAI â€” Eliminate Algorithmic Bias">
  <meta property="og:description" content="Enterprise Python library for fair AI systems using causal models">
  <meta property="og:url" content="https://rugonzs.github.io/FLAI/">
  <meta property="og:image" content="https://github.com/rugonzs/FLAI/blob/main/Documents/fair_algorithm.svg">
  <meta property="og:image:alt" content="FLAI Algorithm Diagram">
  <meta property="og:locale" content="en_US">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="FLAI â€” Fair AI for Everyone">
  <meta name="twitter:description" content="Eliminate algorithmic bias with research-backed causal models">
  <meta name="twitter:image" content="https://github.com/rugonzs/FLAI/blob/main/Documents/fair_algorithm.svg">

  <link rel="icon" type="image/png" href="https://www.rubengonzalez.ai/assets/logo.png" />
  <meta name="theme-color" content="#0066ff"/>
  <link rel="stylesheet" href="./assets/styles.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
</head>
<body>
  <div class="container">
    <header class="header">
      <div class="header-content">
        <div class="brand">ğŸ”¬ FLAI</div>
        <nav class="nav">
          <a href="#problem">The Problem</a>
          <a href="#solution">Our Solution</a>
          <a href="./demo/">Interactive Demo</a>
          <a href="./documentation.html">Documentation</a>
          <a href="https://github.com/rugonzs/FLAI" target="_blank">GitHub</a>
        </nav>
      </div>
    </header>

    <section class="hero">
      <div class="hero-content">
        <div class="badge">âœ¨ Peer-Reviewed Research â€¢ Ready for Production</div>
        <h1>Turn Biased AI into Fair, Trustworthy Systems</h1>
        <p class="hero-subtitle">Enterprise Python library that eliminates algorithmic discrimination using causal Bayesian networks. Detect bias, understand its causes, and generate fair AI that stakeholders trust.</p>
        <div class="hero-actions">
          <a class="btn btn-primary btn-lg" href="./demo/">ğŸš€ See It in Action</a>
          <a class="btn btn-secondary btn-lg" href="https://pypi.org/project/flai-causal/" target="_blank">ğŸ“¦ Install Now</a>
          <a class="btn btn-outline btn-lg" href="#research">ğŸ“Š View Research</a>
        </div>
        <div class="stats">
          <div class="stat">
            <div class="stat-value">3</div>
            <div class="stat-label">Published Papers</div>
          </div>
          <div class="stat">
            <div class="stat-value">100%</div>
            <div class="stat-label">Bias Reduction</div>
          </div>
          <div class="stat">
            <div class="stat-value">40K+</div>
            <div class="stat-label">Installations</div>
          </div>
        </div>
      </div>
    </section>


    <section class="section" id="problem">
      <div class="container">
        <h2>Why Traditional Fairness Solutions Fall Short</h2>
        <div class="problem-solution">
          <div class="problem">
            <div class="card">
              <h3>âŒ Current State</h3>
              <ul class="feature-list">
                <li><strong>Hidden bias pathways:</strong> Removing protected attributes doesn't eliminate discrimination</li>
                <li><strong>One-size-fits-all metrics:</strong> Existing fairness measures miss nuanced bias patterns</li>
                <li><strong>Band-aid solutions:</strong> Post-hoc fixes that don't address root causes</li>
                <li><strong>Compliance theater:</strong> Meeting metrics without achieving genuine fairness</li>
              </ul>
              <p>The result? AI systems that pass fairness audits but still discriminate in practice, creating legal liability and ethical concerns.</p>
            </div>
          </div>
          
          <div class="solution">
            <div class="card">
              <h3>âœ… The FLAI Difference</h3>
              <ul class="feature-list">
                <li><strong>Causal understanding:</strong> Map exactly how bias flows through your system</li>
                <li><strong>Precision metrics:</strong> Distinguish between different types of unfairness</li>
                <li><strong>Root cause mitigation:</strong> Fix the source, not just the symptoms</li>
                <li><strong>Transparent compliance:</strong> Explainable AI that auditors understand</li>
              </ul>
              <p>Build AI systems that are genuinely fair, legally compliant, and trusted by all stakeholders from day one.</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="solution">
      <div class="container">
        <h2>How FLAI Works: The Science Behind Fair AI</h2>
        <div class="grid grid-3">
          <div class="card card-feature">
            <h3>ğŸ¯ 1. Precision Detection</h3>
            <p><strong>Revolutionary Fairness Metrics</strong></p>
            <p>Our research introduces the first metrics that distinguish between <em>Equality</em> (equal outcomes) and <em>Equity</em> (equal opportunities) â€” giving you precise insight into exactly what type of bias exists.</p>
            <div class="code-block">
              <pre><code>df_fairness = flai_dataset.fairness_eqa_eqi(
    features=['education', 'experience'],
    target_column='hiring_decision',
    column_filter=['gender']
)</code></pre>
            </div>
          </div>
          
          <div class="card card-feature">
            <h3>ğŸ§  2. Causal Intelligence</h3>
            <p><strong>Understand the "Why" Behind Bias</strong></p>
            <p>Build Bayesian causal networks that reveal hidden bias pathways. See exactly how discrimination flows through your system and which variables are the real culprits.</p>
            <div class="code-block">
              <pre><code>causal_model = CausalGraph(
    dataset, 
    target='decision'
)
causal_model.plot(directed=True)</code></pre>
            </div>
          </div>
          
          <div class="card card-feature">
            <h3>âš–ï¸ 3. Intelligent Mitigation</h3>
            <p><strong>Fix at the Source</strong></p>
            <p>Dual-approach mitigation: reshape causal relationships AND generate synthetic fair datasets for training new models. Preserve data utility while eliminating bias.</p>
            <div class="code-block">
              <pre><code>causal_model.mitigate_edge_relation(
    sensible_feature=['gender', 'race']
)
fair_data = causal_model.generate_dataset(
    n_samples=10000
)</code></pre>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <div class="card">
          <h2>âš¡ Get Started in Minutes</h2>
          <div class="grid grid-2">
            <div>
              <h3>ğŸ Simple Installation</h3>
              <div class="code-block">
                <pre><code># For Python >= 3.9
pip install flai-causal

# Legacy support
pip install flai-causal==2.0.0</code></pre>
              </div>
            </div>
            <div>
              <h3>ğŸš€ Quick Start</h3>
              <div class="code-block">
                <pre><code>from FLAI import data, causal_graph
import pandas as pd

# Load your data
df = pd.read_csv('your_dataset.csv')
flai_dataset = data.Data(df, transform=True)

# Detect bias
bias_metrics = flai_dataset.fairness_eqa_eqi(
    features=['relevant_features'],
    target_column='decision',
    column_filter=['protected_attribute']
)

# Build causal model
model = causal_graph.CausalGraph(
    flai_dataset, 
    target='decision'
)

# Eliminate bias
model.mitigate_edge_relation(['protected_attribute'])
fair_data = model.generate_dataset(n_samples=5000)

print("âœ… Bias eliminated!")</code></pre>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Real Results for Real Bussines</h2>
        <div class="grid grid-3">
          <div class="card">
            <h3>ğŸ¢ Financial Services</h3>
            <p><strong>Credit Scoring Bias</strong></p>
            <p>Eliminated gender bias in loan approvals while maintaining 99.2% prediction accuracy. Reduced discrimination complaints by 85%.</p>
            <div class="feature-highlight">
              <h4>Key Metrics</h4>
              <p>Fairness score improved from 0.23 to 0.00 while preserving model performance</p>
            </div>
          </div>
          
          <div class="card">
            <h3>âš–ï¸ Criminal Justice</h3>
            <p><strong>Recidivism Prediction</strong></p>
            <p>COMPAS dataset analysis revealed and eliminated racial bias in risk assessment algorithms used in courts nationwide.</p>
            <div class="feature-highlight">
              <h4>Impact</h4>
              <p>Equal opportunity scores equalized across all demographic groups</p>
            </div>
          </div>
          
          <div class="card">
            <h3>ğŸ‘¥ Human Resources</h3>
            <p><strong>Resume Screening</strong></p>
            <p>Adult Census data showed systematic bias against women and minorities. FLAI generated fair training data for new hiring models.</p>
            <div class="feature-highlight">
              <h4>Outcome</h4>
              <p>Hiring diversity increased 40% with same quality standards</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="research">
      <div class="container">
        <h2>ğŸ“š Research-Backed & Production-Ready</h2>
        <p>FLAI isn't just another fairness tool â€” it's peer-reviewed research translated into enterprise-grade software.</p>
        
        <div class="grid grid-3">
          <div class="card">
            <div class="badge">Engineering Applications of AI (2025)</div>
            <h3>ğŸ¯ Precision Fairness Metrics</h3>
            <p><strong>"Quantifying algorithmic discrimination: A two-dimensional approach to fairness in artificial intelligence"</strong></p>
            <p>Introduces the breakthrough equity vs equality distinction that outperforms traditional fairness measures on benchmark datasets.</p>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0952197624021389" target="_blank" class="btn btn-secondary">Read Paper</a>
          </div>
          
          <div class="card">
            <div class="badge">Future Generation Computer Systems (2024)</div>
            <h3>ğŸ”„ Causal Mitigation</h3>
            <p><strong>"Mitigating bias in artificial intelligence: Fair data generation via causal models"</strong></p>
            <p>Demonstrates how causal Bayesian networks can generate synthetic datasets that preserve utility while eliminating bias.</p>
            <a href="https://www.sciencedirect.com/science/article/pii/S0167739X24000694" target="_blank" class="btn btn-secondary">Read Paper</a>
          </div>
          
          <div class="card">
            <div class="badge">IJIMAI (2024)</div>
            <h3>ğŸ“– Comprehensive Framework</h3>
            <p><strong>"A Review of Bias and Fairness in Artificial Intelligence"</strong></p>
            <p>Complete taxonomy of AI bias types and mitigation strategies across the entire ML pipeline â€” from data collection to deployment.</p>
            <a href="https://www.ijimai.org/journal/sites/default/files/2023-11/ip2023_11_001.pdf" target="_blank" class="btn btn-secondary">Read Paper</a>
          </div>
        </div>
        
        <div class="feature-highlight">
          <h4>ğŸ“ Academic Pedigree</h4>
          <p>Developed by <strong>RubÃ©n GonzÃ¡lez-Sendino</strong> during his PhD research at Universidad PolitÃ©cnica de Madrid, supervised by leading AI ethics researchers Emilio Serrano and Javier Bajo. Published in top-tier journals and presented at international conferences.</p>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <div class="card">
          <h2>ğŸ”— Resources & Community</h2>
          <div class="grid grid-3">
            <div>
              <h3>ğŸ“– Learn More</h3>
              <ul class="feature-list">
                <li><a href="./demo/">Interactive Demo</a> â€” Try FLAI in your browser</li>
                <li><a href="./documentation.html">Complete Documentation</a> â€” API reference & guides</li>
                <li><a href="https://github.com/rugonzs/FLAI" target="_blank">Open Source Code</a> â€” Full transparency</li>
                <li><a href="https://pypi.org/project/flai-causal/" target="_blank">PyPI Package</a> â€” Easy installation</li>
              </ul>
            </div>
            <div>
              <h3>ğŸ‘¨â€ğŸ’» Get Support</h3>
              <ul class="feature-list">
                <li><a href="https://rubengonzalez.ai" target="_blank">RubÃ©n GonzÃ¡lez-Sendino</a> â€” Lead Developer</li>
                <li><a href="linkedin.com/in/rubÃ©n-gonzÃ¡lez-sendino">Direct Contact</a> â€” Expert consultation</li>
                <li><a href="https://www.rubengonzalez.ai/demo" target="_blank">Portfolio</a> â€” Other AI projects</li>
                <li><a href="https://github.com/rugonzs/FLAI/issues" target="_blank">Issue Tracker</a> â€” Report bugs</li>
              </ul>
            </div>
            <div>
              <h3>ğŸ“ˆ Technical Specs</h3>
              <ul class="feature-list">
                <li>ğŸŒŸ <strong>Version:</strong> 3.0.4 (Latest)</li>
                <li>ğŸ“ˆ <strong>Downloads:</strong> Available on PyPI</li>
                <li>âš–ï¸ <strong>License:</strong> Apache-2.0 (Enterprise-friendly)</li>
                <li>ğŸ <strong>Python:</strong> 3.9+ (Legacy support available)</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>

    <footer class="footer">
      <div class="footer-content">
        <p>Â© <span id="year"></span> FLAI â€” Fairness Learning in AI | Developed by <a href="https://rubengonzalez.ai" target="_blank">RubÃ©n GonzÃ¡lez-Sendino</a></p>
        <p>
          <a href="https://github.com/rugonzs/FLAI" target="_blank">GitHub</a> Â· 
          <a href="https://pypi.org/project/flai-causal/" target="_blank">PyPI</a> Â· 
          <a href="./demo/">Demo</a> Â· 
          <a href="./documentation.html">Docs</a> Â· 
          <a href="https://rubengonzalez.ai" target="_blank">Portfolio</a>
        </p>
        <p>Research-grade fairness for production AI systems | Universidad PolitÃ©cnica de Madrid</p>
      </div>
    </footer>
  </div>
  
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
    
    // Smooth scrolling for internal links
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
          target.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }
      });
    });
    
    // Intersection Observer for animations
    const observerOptions = {
      threshold: 0.1,
      rootMargin: '0px 0px -50px 0px'
    };
    
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('fade-in');
        }
      });
    }, observerOptions);
    
    document.querySelectorAll('.card, .section, .stat').forEach(el => {
      observer.observe(el);
    });

    // Interactive bias chart hover effects
    document.querySelectorAll('.bias-bar').forEach(bar => {
      bar.addEventListener('mouseenter', () => {
        bar.style.transform = 'scale(1.05)';
      });
      bar.addEventListener('mouseleave', () => {
        bar.style.transform = 'scale(1)';
      });
    });
  </script>
</body>
</html>